- Подготовить источники данных вне облака

В качестве источников данных будем использовать файлы в формате csv, взятые, например, из других модулей курса:

[ovidp-covid-data.csv](owid-covid-data.csv)

- Развернуть аналитическую СУБД в облаке

Для упрощенного варианта пайплайна выберем аналитическую БД Clickhouse из того соображения, что с ней я уже работал ранее на курсе. Соответственно, нам понадобятся:

1. Средство ETL обработки - будем использовать Zeppelin для загрузки csv файла через Data Proc Apache Spark в Managed Service for Clickhouse
2. Для хранения и обработки удобным будет Data Proc: Apache Spark, а для дашбордов - Managed Service for Clickhouse
3. Для визуализации воспользуемся дашбордами DataLens

- Организовать загрузку данных из источников в аналитическую СУБД

1. Установлен кластер Data Proc
![img.png](img.png)
2. Установлен и настроен кластер Managed Service for Clickhouse
![img_1.png](img_1.png)
3. Файл [owid-covid-data.csv](owid-covid-data.csv) загружен в clickhouse db через Zeppelin (который имеется в составе Data Proc)
![img_2.png](img_2.png)
Проверил, что данные появились в clickhouse кластере в db
![img_3.png](img_3.png)
- Создать витрины
- Создать дашборды
Настроил подключение к DataLens
![img_4.png](img_4.png)
![img_5.png](img_5.png)
В DataLens создал все, что позовлял его интерфейс в моем случае
![img_6.png](img_6.png)
![img_7.png](img_7.png)
![img_8.png](img_8.png)
Должен отметить, что добавление чарта на дашборд хотя и прошло успешно, но при сохранении вылетает ошибка с предложением обратиться в техподдержку, пока не стал с этим разбираться, так как это выходит за рамки задачи (какой-то баг в Yandex)